<!DOCTYPE html><html lang="en-us"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><title># [论文笔记] FGSM-Explaining and Harnessing Adversarial Examples | ヾ(•ω•`)o Shammy</title><meta name="description"><meta name="generator" content="ヾ(•ω•`)o Shammy"><meta name="author" content="John Doe"><meta name="keywords"><meta name="HandheldFriendly" content="True"><meta name="MobileOptimized" content="320"><meta name="viewport" content="width=device-width,initial-scale=1.0,maximum-scale=1,user-scalable=0"><link rel="stylesheet" type="text/css" href="/styles/screen.css"><link rel="apple-touch-icon" sizes="57x57" href="/images/apple-touch-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/images/apple-touch-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/images/apple-touch-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/images/apple-touch-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/images/apple-touch-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/images/apple-touch-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/images/apple-touch-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/images/apple-touch-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-180x180.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/images/favicon-96x96.png"><link rel="icon" type="image/png" sizes="128x128" href="/images/favicon-128.png"><link rel="icon" type="image/png" sizes="196x196" href="/images/favicon-196x196.png"><meta name="msapplication-TileColor" content="#FFFFFF"><meta name="msapplication-TileImage" content="mstile-144x144.png"><meta name="msapplication-square70x70logo" content="mstile-70x70.png"><meta name="msapplication-square150x150logo" content="mstile-150x150.png"><meta name="msapplication-wide310x150logo" content="mstile-310x150.png"><meta name="msapplication-square310x310logo" content="mstile-310x310.png"><link rel="stylesheet" href="/css/prism.css" type="text/css"></head><body itemscope itemtype="https://schema.org/WebPage"><header itemscope itemtype="https://schema.org/WPHeader"><div class="logo"></div><h1><a href="/" alt="ヾ(•ω•`)o Shammy" title="ヾ(•ω•`)o Shammy" itemprop="headline">ヾ(•ω•`)o Shammy</a></h1><p itemprop="description"></p><nav itemscope itemtype="https://schema.org/SiteNavigationElement"><ul><li itemprop="name"><a href="/ " alt="home" title="home" itemprop="url">home</a></li><li itemprop="name"><a href="/articles" alt="articles" title="articles" itemprop="url">articles</a></li><li itemprop="name"><a href="/author" alt="author" title="author" itemprop="url">author</a></li></ul></nav><div class="space"></div></header><main itemscope itemtype="https://schema.org/Blog"><article class="full"><h1 itemprop="headline" class="post-heading"># [论文笔记] FGSM-Explaining and Harnessing Adversarial Examples</h1><span class="page-tag-anchor"><a href="/tags/FGSM" itemprop="url">#FGSM
&nbsp;&nbsp;</a></span><span class="page-tag-anchor"><a href="/tags/Adversarial Examples" itemprop="url">#Adversarial Examples
&nbsp;&nbsp;</a></span><span class="page-tag-anchor"><a href="/tags/Adversarial training" itemprop="url">#Adversarial training
&nbsp;&nbsp;</a></span><span class="post-meta"></span><br><br><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<h2 id="1-INTRODUCTION"><a href="#1-INTRODUCTION" class="headerlink" title="1 INTRODUCTION"></a>1 INTRODUCTION</h2><blockquote>
<p><em>Adversarial training can provide an additional regularization benefit beyond that provided by using dropout (Srivastava et al., 2014) alone. Generic regularization strategies such as dropout, pretraining, and model averaging do not confer a significant reduction in a model’s vulnerability to adversarial examples, but changing to nonlinear model families such as RBF networks can do so.</em>  </p>
</blockquote>
<p>除了使用dropout所产生的收益外，对抗训练可以提供一个<em>额外的正则化收益</em>。一般的正则化方法：dropout，预训练和模型平均，不能明显提高模型对于对抗样本的鲁棒性。将模型转换成非线性的，比如RBF网络，可以降低模型的脆弱性。 </p>
<blockquote>
<p><em>A fundamental tension between designing models that are easy to train due to their linearity and designing models that use nonlinear effects to resist adversarial perturbation.</em>  </p>
</blockquote>
<p>线性模型容易训练，但是容易受攻击；非线性模型较难训练，但是能够抵御对抗扰动。设计模型时应权衡两者。  </p>
<h2 id="2-RELATED-WORK"><a href="#2-RELATED-WORK" class="headerlink" title="2 RELATED WORK"></a>2 RELATED WORK</h2><p>Szegedy et al.(2014b) 发现了神经网络及相关模型的一些有趣的性质：  </p>
<ul>
<li>Box-constrained L-BFGS 可以可靠地找到对抗样本；  </li>
<li>在ImageNet等数据集上，对抗样本与原始样本非常接近，人眼无法察觉它们的差异；  </li>
<li>不同结构的分类器或是在不同子集上训练的分类器也会误判同一个对抗样本；</li>
<li>Shallow softmax regression model 也容易受对抗样本的攻击；</li>
<li>对抗训练对模型有正则化作用（现在还不实用）。</li>
</ul>
<h2 id="3-THE-LINEAR-EXPLANATION-OF-ADVERSARIAL-EXAMPLES"><a href="#3-THE-LINEAR-EXPLANATION-OF-ADVERSARIAL-EXAMPLES" class="headerlink" title="3 THE LINEAR EXPLANATION OF ADVERSARIAL EXAMPLES"></a>3 THE LINEAR EXPLANATION OF ADVERSARIAL EXAMPLES</h2><blockquote>
<p><em>高维问题的n非常大，扰动\(η\)的每个元素可以非常小，但是\(w^T{\eta}\)会累加所有微小改变，增量很大。</em>  </p>
</blockquote>
<p>当扰动<strong><em>η</em></strong>的每个元素＜对应特征的精确度时，如果分类器对<strong><em>x</em></strong>和<strong><em>\(\tilde{x}\)=x+η</em></strong>的分类结果不同，这是不合理的。对于具有良好分离类别的问题来说，只要<strong><em>η</em></strong>满足\(||η||_∞&lt;{\epsilon}\)，（其中\({\epsilon}\)小到被问题的感受器或数据存储设备抛弃），我们期望分类器对<strong><em>x</em></strong>和\(\tilde{x}\)的分类是相同的。<br>$$w^T{\tilde{x}=w^Tx+w^T{\eta}}$$  </p>
<p>上式是权重向量\(w\)和对抗样本\(\tilde{x}\)的点乘。对抗扰动导致activation增大了\(w^T{\eta}\)。<br>令\({\eta}=sign(w)\)，使得\({\eta}\)上有最大正则限制，可以最大化对activation的增量。 如果权重向量w有n个维度，且元素的平均值为m，那么activation会增大\({\epsilon}mn\)。因为\(||η||_∞\)不会随着维度n增大，但是扰动η引起的activation增量会随维度n线性增大，所以对于一个高维问题，可以对输入的各个元素（特征）进行无限小的改变，而最后的输出会有一个巨大改变。  </p>
<h2 id="4-LINEAR-PERTURBATION-OF-NON-LINEAR-MODELS"><a href="#4-LINEAR-PERTURBATION-OF-NON-LINEAR-MODELS" class="headerlink" title="4 LINEAR PERTURBATION OF NON-LINEAR MODELS"></a>4 LINEAR PERTURBATION OF NON-LINEAR MODELS</h2><p>Section 3 说明存在一个快速产生对抗样本的方法。假设神经网络非常接近线性，因而无法抵御线性对抗扰动。LSTMs(Hochreiter &amp; Schmidhuber, 1997)，ReLUs(Jarrett et al., 2009; Glorot et al.,2011)和maxout networks(Goodfellow et al.,2013c)都被刻意设计成非常线性的网络，所以它们易于优化。出于同样的原因，对许多非线性模型，比如sigmoid networks进行了仔细调整，使其更多时候处于非饱和、更线性的状态。这种线性行为表明，线性模型的代价小、可分析的扰动也应能够破坏神经网络。   </p>
<p>令\({\theta}\)为模型参数，\(x\)为模型输入，\(y\)为\(x\)的关联输出，\(J({\theta},x,y)\)为用于训练神经网络的代价函数。可以围绕\({\theta}\)的当前值来线性化代价函数，得到一个最优的最大范数限制扰动，为：<br>$${\eta}={\epsilon}sign(▽_xJ({\theta},x,y))$$<br>上式即快速梯度符号方法（Fast gradient sign method, FGSM）,用于生成对抗样本。使用反向传播可以高效地计算出所求的梯度。</p>
<h2 id="5-ADVERSARIAL-TRAINING-OF-LINEAR-MODELS-VERSUS-WEIGHT-DECAY"><a href="#5-ADVERSARIAL-TRAINING-OF-LINEAR-MODELS-VERSUS-WEIGHT-DECAY" class="headerlink" title="5 ADVERSARIAL TRAINING OF LINEAR MODELS VERSUS WEIGHT DECAY"></a>5 ADVERSARIAL TRAINING OF LINEAR MODELS VERSUS WEIGHT DECAY</h2><p>通过<strong>逻辑回归</strong>的例子学习对抗样本是如何生成的。<br>训练单个模型，输出标签\(y∈\{-1，1\}\)且\(P(y=1)={\sigma}(w^Tx+b)\)，其中\({\sigma}(z)\)是logistic sigmoid 函数。<br>$$E_{x,y～p_{data}}{\zeta}(-y(w^Tx+b))$$<br>训练由上式中的梯度下降组成。 其中\({\zeta(z)=log(1+exp(z))}\)是softplus函数。可以基于梯度符号扰动，推导出一个在\(x\)的最坏情况下的扰动上进行训练的简单解析式，而不是\(x\)本身。<br>注意，梯度的符号仅仅是\(-sign(n)\)，并且\(w^Tsign(w)=||w||_1\)。因此，逻辑回归的对抗版本是最小化</p>
<p>$$E_{x,y～p_{data}}{\zeta}(y({\epsilon}||w||_1-w^Tx-b))$$  </p>
<p>在某种程度上，这和L1正则化很相似，但是它们之间有着重要区别。L1正则化的训练过程中，是从模型的activation中减去L1惩罚项，而非将其加到训练代价中。</p>
<h2 id="6-ADVERSARIAL-TRAINING-OF-DEEP-NETWORKS"><a href="#6-ADVERSARIAL-TRAINING-OF-DEEP-NETWORKS" class="headerlink" title="6 ADVERSARIAL TRAINING OF DEEP NETWORKS"></a>6 ADVERSARIAL TRAINING OF DEEP NETWORKS</h2><p>与浅层线性模型不同，深度网络至少能够表示抵御对抗扰动的函数。通用近似理论保证，对于具有一个以上隐藏层的神经网络，只要隐藏层具有足够多的单元，该网络就可以表示任意精度的函数。浅层线性模型不能在使不同训练点有不同输出的同时，又在训练点附近成为常数。  </p>
<p>Szegedy et al. 表示，在混合了对抗样本和干净样本的数据集上进行训练，对神经网络有一定程度的正则化作用。  </p>
<p>在基于FGSM的对抗目标函数上训练，是一个有效的正则化方法：<br>$$\tilde{J}({\theta},x,y)={\alpha}J({\theta},x,y)+(1-{\alpha})J({\theta},x+{\epsilon}sign(▽_xJ({\theta},x,y)))$$</p>
</article><br><br><span class="next-post"><a href="/2020/01/10/first-article/" itemprop="url">Older Post ⇒</a></span><br><br><br></main></body></html>